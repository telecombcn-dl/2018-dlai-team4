# -*- coding: utf-8 -*-
"""Transfer_learning

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1w07uOcZ-kumujcxsTmI5A5_UhWGaGC8n

# Import data from github
"""

size_of_output=100                #square image so size_of_output*size_of_output
#nomenclature : "filtered_pict_" + how to choose the people (number of picture each people should have at least) + number of pict kept for each person (at maximum)
folder_to_use="filtered_pict_20"
#folder_to_use="filtered_pict_10_every-picture" 
#folder_to_use="filtered_pict_10_10" 
RGB_output=True
run_preprocessing = True

!git clone https://github.com/telecombcn-dl/2018-dlai-team4.git
import os
os.chdir("2018-dlai-team4")


from import_picture import import_processed_pict_from


(x_train, y_train), (x_val, y_val), (x_test, y_test),name_list=import_processed_pict_from(folder_to_use,size_of_output,RGB_output)

import numpy as np
x_train=np.asarray(x_train)
x_test=np.asarray(x_test)
x_val=np.asarray(x_val)
if not RGB_output: 
  x_train=x_train.reshape(len(x_train),size_of_output,size_of_output,1)
  x_test=x_test.reshape(len(x_test),size_of_output,size_of_output,1)
  x_val=x_val.reshape(len(x_val),size_of_output,size_of_output,1)

from keras.utils import to_categorical
y_train = to_categorical(y_train)
y_val = to_categorical(y_val)
y_test = to_categorical(y_test)

ind = np.arange(len(x_train))
np.random.shuffle(ind)
x_train = x_train[ind]
y_train = y_train[ind]

"""# **Transfer Learning - Feature extraction**

Use VGG19 as pretrain network.
"""

from keras.applications import VGG19

conv_base = VGG19(weights='imagenet',
                  include_top=False,
                  input_shape=(100, 100, 3))

"""resize the training, validation and test data to fit the output of pre-trained network."""

import os
import numpy as np
from keras.preprocessing.image import ImageDataGenerator


datagen = ImageDataGenerator()
batch_size = 16

def extract_features(x, y, sample_count):
    features = np.zeros(shape=(sample_count, 3, 3, 512))
    labels = np.zeros(shape=(sample_count,62))
    generator = datagen.flow(x,y, batch_size=batch_size)

    i = 0
    for inputs_batch, labels_batch in generator:
        features_batch = conv_base.predict(inputs_batch)
        features[i * batch_size : (i + 1) * batch_size] = features_batch   
        labels[i * batch_size : (i + 1) * batch_size] = labels_batch
        i += 1
        if i * batch_size >= sample_count:

            break
            
    return features, labels 

train_features, train_labels = extract_features(x_train, y_train, len(y_train))
validation_features, validation_labels = extract_features(x_val, y_val, len(y_val))
test_features, test_labels = extract_features(x_test, y_test, len(y_test))

"""Flatten the features"""

train_features = np.reshape(train_features, (len(y_train), 3 * 3 * 512))
validation_features = np.reshape(validation_features, (len(y_val), 3 * 3 * 512))
test_features = np.reshape(test_features, (len(y_val), 3 * 3 * 512))

from keras.callbacks import EarlyStopping, ModelCheckpoint


#Many other options for the early stopping are available. In my opinion this should be enough for our purposes
callback = [EarlyStopping(monitor='val_acc', patience=15),
             ModelCheckpoint(filepath='best_model.h5', monitor='val_acc', save_best_only=True)]

"""Add Dense layers and train the model"""

from keras import models
from keras import layers
from keras import optimizers

model = models.Sequential()
model.add(layers.Dense(512, activation='relu', input_dim=3 * 3 * 512))
model.add(layers.Dense(256))
model.add(layers.Dropout(0.5))
model.add(layers.Dense(62, activation='softmax'))

model.compile(optimizer=optimizers.RMSprop(lr=2e-5),
              loss='categorical_crossentropy',
              metrics=['acc'])

history = model.fit(train_features, train_labels,
                    epochs=250,
                    batch_size=16,
                    validation_data=(validation_features, validation_labels))

model.load_weights('best_model.h5')

"""test the model"""

test_loss, test_acc = model.evaluate(test_features, test_labels)
print("Accuracy = " + str(test_acc))

"""# **Transfer Learning - Fine Tuning**

Unfreeze the 5th Block
"""

conv_base.trainable = True

set_trainable = False

for layer in conv_base.layers:
    if layer.name == 'block5_conv1':
        set_trainable = True
    if set_trainable:
    else:
        layer.trainable = False

"""Define the Dense layers"""

from keras import models
from keras import layers

model = models.Sequential()
model.add(conv_base)
model.add(layers.Flatten())
model.add(layers.Dense(512, activation='relu'))
model.add(layers.Dense(256, activation='relu'))
model.add(layers.Dropout(0.5))
model.add(layers.Dense(62, activation='softmax'))

model.summary()

"""Add data augmentation"""

from keras.preprocessing.image import ImageDataGenerator
datagen = ImageDataGenerator(
    #rotation_range = 15,       #useful (smaller than 20 seems better)
    width_shift_range = 0.1,   #useful for small values
    #height_shift_range = 0.1, #not useful
    #shear_range = 0.1,        #not useful
    zoom_range = 0.2,         #useful
    horizontal_flip = True,   #useful
    #vertical_flip = True
    #fill_mode = 'nearest'
    )

"""Early stopping"""

from keras.callbacks import EarlyStopping, ModelCheckpoint


#Many other options for the early stopping are available. In my opinion this should be enough for our purposes
callback = [EarlyStopping(monitor='val_acc', patience=15),
             ModelCheckpoint(filepath='best_model.h5', monitor='val_acc', save_best_only=True)]

"""Build and train the network"""

from keras import optimizers
model.compile(optimizer=optimizers.RMSprop(lr=2e-5),
              loss='categorical_crossentropy',
              metrics=['acc'])

history = model.fit_generator(datagen.flow(x_train, y_train ,batch_size = 16,shuffle = True),  #batch size was 32 
                    epochs=100,
                    shuffle = True, 
                    steps_per_epoch =  len(x_train)/32,  # steps were /32
                    validation_data = (x_val, y_val),
                    validation_steps=50,
                    callbacks = callback
                    )

model.load_weights('best_model.h5')

test_loss, test_acc = model.evaluate(x_test, y_test)
print(test_acc)