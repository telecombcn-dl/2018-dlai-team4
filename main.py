# -*- coding: utf-8 -*-
"""Copy of First.ipyn

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1eqZE2md_MkwbpPLum0aCG0P6oQgTMU0b

# Import data from github repository IF YOU DON'T KNOW WHAT YOU DO RUN THIS ONE
"""

# decide model
model = models.Sequential()

# three convolutional layers
model.add(layers.Conv2D(32, (3, 3), activation='relu',
                        batch_input_shape=(64, 40, 430, 1)))
model.add(layers.MaxPooling2D(2, 2))
model.add(layers.Conv2D(32, (3, 3), activation='relu'))
model.add(layers.MaxPooling2D(2, 2))
model.add(layers.Conv2D(64, (3, 3), activation='relu'))

# CNN to RNN
model.add(layers.Reshape(target_shape=(6, 104*64)))
model.add(layers.Dense(64, activation='relu'))

model.summary()

# RNN layer
model.add(layers.LSTM(64, return_sequences=True, stateful=True,
                      batch_input_shape=(64, 6, 64)))

# convert 3D to 1D
model.add(layers.Flatten())


# fully-connected layer
model.add(layers.Dense(64, activation='relu'))
model.add(layers.Dropout(0.5))
model.add(layers.Dense(5, activation='softmax'))

size_of_output=100                #square image so size_of_output*size_of_output
#nomenclature : "filtered_pict_" + how to choose the people (number of picture each people should have at least) + number of pict kept for each person (at maximum)
#folder_to_use="filtered_pict_20"
folder_to_use="filtered_pict_20_every-picture"
#folder_to_use="filtered_pict_10_every-picture" 
#folder_to_use="filtered_pict_10_10" 
#folder_to_use="optimal_preprocessing/filtered_pict_29_20"
RGB_output=True
run_preprocessing = True

!git clone https://github.com/telecombcn-dl/2018-dlai-team4.git
import os
os.chdir("2018-dlai-team4")


from import_picture import import_processed_pict_from


(x_train, y_train), (x_val, y_val), (x_test, y_test),name_list=import_processed_pict_from(folder_to_use,size_of_output,RGB_output)
#Here we just perform the needed reshaping (so that all values are in the [0,1] interval), we switch the labels to categorical and perform random shuffle of the input data.
import numpy as np
x_train=np.asarray(x_train)
x_test=np.asarray(x_test)
x_val=np.asarray(x_val)
if not RGB_output: 
  x_train=x_train.reshape(len(x_train),size_of_output,size_of_output,1)
  x_test=x_test.reshape(len(x_test),size_of_output,size_of_output,1)
  x_val=x_val.reshape(len(x_val),size_of_output,size_of_output,1)

from keras.utils import to_categorical
y_train = to_categorical(y_train)
y_val = to_categorical(y_val)
y_test = to_categorical(y_test)

ind = np.arange(len(x_train))
np.random.shuffle(ind)
x_train = x_train[ind]
y_train = y_train[ind]

len(y_train)

"""ADRIANO if you want to create new folder DO NOT RUN the first one"""

all_per_person = True 
everyone = False 
treshold= 130 #only the people with more than treshold picture will be kept
number_of_pict_to_keep=10 #number of pict keep per person 

!git clone https://github.com/telecombcn-dl/2018-dlai-team4.git
import os
os.chdir("2018-dlai-team4")



!pip install xlrd

from only_keep_good_files import create_new_parse


create_new_parse(treshold, number_of_pict_to_keep,all_per_person,everyone)



from import_picture import import_processed_pict_from

size_of_output=100                #square image so size_of_output*size_of_output
folder_to_use="filtered_pict_"+str(treshold)+"_"+str(number_of_pict_to_keep)

(x_train, y_train), (x_val, y_val), (x_test, y_test),name_list=import_processed_pict_from(folder_to_use,size_of_output)

import numpy as np
x_train=np.asarray(x_train)
x_test=np.asarray(x_test)
x_val=np.asarray(x_val)
x_train=x_train.reshape(len(x_train),size_of_output,size_of_output,1)
x_test=x_test.reshape(len(x_test),size_of_output,size_of_output,1)
x_val=x_val.reshape(len(x_val),size_of_output,size_of_output,1)

os.chdir("George_W_Bush")
os.listdir()

"""# Import data from github repository and pre-process DETAIL , DO NOT TOUCH EXCEPT IF YOU HAVE A REALLY GOOD REASON

test if we have the folder
"""

import keras
import numpy as np
np.random.seed(123)

!dir

"""If first time"""

!git clone https://github.com/telecombcn-dl/2018-dlai-team4.git

"""RUN EVERYTHING FROM HERE 
change the current folder
"""

import os
os.chdir("2018-dlai-team4")

"""import and pre-process data"""

!pip install xlrd
from only_keep_good_files import create_new_parse

#treshold >= number_of_pict_to_keep
treshold= 130 #only the people with more than treshold picture will be kept
number_of_pict_to_keep=10 #number of pict keep per person 


create_new_parse(treshold, number_of_pict_to_keep)

from import_picture import import_processed_pict_from

size_of_output=100  #square image so size_of_output*size_of_output

(x_train, y_train), (x_val, y_val), (x_test, y_test),name_list=import_processed_pict_from("few_pict_for_test",size_of_output)

"""verification

# **First attempt: applying a basic convNet (Initial part of the code. not useful anymore)**
"""

from keras import layers
from keras import models
from keras import regularizers



output_size=len(y_train[1])
print("output size automatisation : "+ str(output_size))
#kernel_regularizer = regularizers.l2(0.001)
input_size=x_train[1].shape
print("input size automatisation : "+ str(input_size))

#Any layer with "added" written on its right means it was not part of the initial structure. Then again the initial structure was built more or less intuitively.
model = models.Sequential()
model.add(layers.Conv2D(32, (3, 3), activation='relu',kernel_regularizer = regularizers.l2(0.001), input_shape=input_size))
#model.add(layers.Conv2D(32, (3, 3), activation='relu',kernel_regularizer = regularizers.l2(0.001), input_shape=(100,100,1)))
model.add(layers.Conv2D(32, (3, 3), activation='relu',kernel_regularizer = regularizers.l2(0.001)))  #added
model.add(layers.MaxPooling2D((2, 2)))
model.add(layers.Conv2D(64, (3, 3),kernel_regularizer = regularizers.l2(0.001), activation='relu'))
model.add(layers.MaxPooling2D((2, 2)))
model.add(layers.Conv2D(64, (3, 3), kernel_regularizer = regularizers.l2(0.001),activation='relu'))
model.add(layers.Flatten())
#model.add(layers.Dense(1024, kernel_regularizer = regularizers.l2(0.001),activation='relu'))
#model.add(layers.Dense(1024, kernel_regularizer = regularizers.l2(0.001),activation='relu'))
model.add(layers.Dense(output_size, kernel_regularizer = regularizers.l2(0.001),activation='softmax'))

model.summary()

from keras import optimizers
from keras.optimizers import SGD
batch_S = 32
epoc = 100

#opt = SGD(lr=0.1)    #DO NOT USE. Employed for the first tests and to check that it actually showed convergence issues
opt = optimizers.SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)  
model.compile(optimizer=opt,
              loss='categorical_crossentropy',
              metrics=['accuracy'])

history = model.fit(x_train, y_train, epochs=epoc,shuffle = True, batch_size=batch_S, validation_data = (x_val,y_val))

"""Now we plot the evolution of training and validation accuracy through the epocs"""

import matplotlib.pyplot as plt

acc = history.history['acc']
val_acc = history.history['val_acc']
loss = history.history['loss']
val_loss = history.history['val_loss']

epochs = range(1, len(acc) + 1)

# "bo" is for "blue dot"
plt.plot(epochs, loss, 'bo', label='Training loss')
# b is for "solid blue line"
plt.plot(epochs, val_loss, 'b', label='Validation loss')
plt.title('Training and validation loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()

plt.show()

"""And finally evaluate the performance over the test set"""

test_loss, test_acc = model.evaluate(x_test, y_test)
test_acc

"""# **CNN with data augmentation**"""

import keras.backend as K


def precision(y_true, y_pred):
    '''Calculates the precision, a metric for multi-label classification of
    how many selected items are relevant.
    '''
    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))
    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))
    precision = true_positives / (predicted_positives + K.epsilon())
    return precision


def recall(y_true, y_pred):
    '''Calculates the recall, a metric for multi-label classification of
    how many relevant items are selected.
    '''
    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))
    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))
    recall = true_positives / (possible_positives + K.epsilon())
    return recall

from keras import layers
from keras import models
from keras import regularizers



output_size=len(y_train[1])
print("output size automatisation : "+ str(output_size))
input_size=x_train[1].shape
print("input size automatisation : "+ str(input_size))

model = models.Sequential()
model.add(layers.Conv2D(32, (3, 3), activation='relu',kernel_regularizer = regularizers.l2(0.001), input_shape=input_size))
model.add(layers.Conv2D(32, (3, 3), kernel_regularizer = regularizers.l2(0.001),activation='relu'))  
model.add(layers.Conv2D(32, (3, 3), kernel_regularizer = regularizers.l2(0.001),activation='relu')) 
model.add(layers.MaxPooling2D((2, 2)))
model.add(layers.Conv2D(64, (3, 3),kernel_regularizer = regularizers.l2(0.001), activation='relu'))
model.add(layers.Conv2D(64, (3, 3),kernel_regularizer = regularizers.l2(0.001), activation='relu'))   
model.add(layers.MaxPooling2D((2, 2)))
model.add(layers.Conv2D(128, (3, 3), kernel_regularizer = regularizers.l2(0.001),activation='relu')) 
model.add(layers.Conv2D(128, (3, 3), kernel_regularizer = regularizers.l2(0.001),activation='relu')) 
model.add(layers.Flatten())
model.add(layers.Dropout(0.5))    
model.add(layers.Dense(output_size, kernel_regularizer = regularizers.l2(0.001),activation='softmax'))  

  
  

from keras import optimizers
from keras.optimizers import SGD
#a lot of epochs but we are using early stopping. With our settings we never actually reach such value
epoc = 500

#The optimizer is the same one of the other network
opt = optimizers.SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)  

model.compile(optimizer=opt,
            loss='categorical_crossentropy',
            metrics=['accuracy',precision,recall])

model.summary()

"""Apply early stopping"""

from keras.callbacks import EarlyStopping, ModelCheckpoint


#We just chose accuracy as the early stopping criterion. Of course also the loss was an option. 
#The value of patience is kind of high but the training is relatively fast so it is not an issue. 
callback = [EarlyStopping(monitor='val_acc', patience=20),
             ModelCheckpoint(filepath='best_model.h5', monitor='val_acc', save_best_only=True)]

"""Data augmentation"""

#The kind of augmentations we perform depend on the dataset (it does not make much sense to emply vertical_flip for example).
#All parameters (commented or not) have been tried. On their right we show whether they are useful or not and for which values.
from keras.preprocessing.image import ImageDataGenerator
datagen = ImageDataGenerator(
    rotation_range = 15,       #useful (smaller than 20 seems better)
    width_shift_range = 0.1,   #useful for small values
    #height_shift_range = 0.1, #not useful
    #shear_range = 0.1,        #not useful
    zoom_range = 0.2,         #useful
    horizontal_flip = True,   #useful
    #vertical_flip = True
    #fill_mode = 'nearest'
    )

"""Train the model"""

#This part was used to perform the training without data augmentation (in orde to see the difference in performance)
history = model.fit(x_train, y_train,batch_size = 128,   
                    epochs=epoc,
                    shuffle = True, 
                    validation_data = (x_val,y_val),
                    callbacks = callback)

#Training with data augmentation
history = model.fit_generator(datagen.flow(x_train, y_train,batch_size = 128,shuffle = True),  
                    epochs=epoc,
                    shuffle = True, 
                    steps_per_epoch =  len(x_train)/32,  
                    validation_data = (x_val,y_val),
                    callbacks = callback,
                    validation_steps = 50)

#Plots for accuracy and loss on both training and validation set
import matplotlib.pyplot as plt

acc = history.history['acc']
val_acc = history.history['val_acc']
loss = history.history['loss']
val_loss = history.history['val_loss']

epochs = range(1, len(acc) + 1)


# "bo" is for "blue dot"
plt.plot(epochs, acc, 'bo', label='Training accuracy')
# b is for "solid blue line"
plt.plot(epochs, val_acc, 'b', label='Validation accuracy')
plt.title('Training and validation accuracy')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend()

plt.show()

# "bo" is for "blue dot"
plt.plot(epochs, loss, 'bo', label='Training loss')
# b is for "solid blue line"
plt.plot(epochs, val_loss, 'b', label='Validation loss')
plt.title('Training and validation loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()

plt.show()

"""Load the weights the best weights for the model and compute the test accuracy"""

#Since we are using early stopping we need to load the weights for the best case.
model.load_weights('best_model.h5')
print(model.metrics_names)
test_loss, test_acc, test_precision , test_recall= model.evaluate(x_test, y_test)
print("Accuracy = " + str(test_acc))
print("Precision = " + str(test_precision))
print("Recall = " + str(test_recall))

"""# Transfer Learning"""

#To perform transfer learning we use the VGG19 model
from keras.applications import VGG19

base_model = VGG19(weights='imagenet',
                  include_top=False,
                  input_shape=(100, 100, 3))

base_model.summary()

from keras import models
from keras import layers

output_size=len(y_train[1])

for layer in base_model.layers:
    layer.trainable = False

model = models.Sequential()
model.add(base_model)
model.add(layers.Flatten())
model.add(layers.Dense(512, activation='relu'))
model.add(layers.Dropout(0.5)) 
model.add(layers.Dense(output_size, activation='softmax'))

model.summary()

"""Data Augmentation"""

#The kind of augmentations we perform depend on the dataset (it does not make much sense to emply vertical?flip for example).

from keras.preprocessing.image import ImageDataGenerator
datagen = ImageDataGenerator(
    rotation_range = 15,       #useful (smaller than 20 seems better)
    width_shift_range = 0.1,   #useful for small values
    #height_shift_range = 0.1, #not useful
    #shear_range = 0.1,        #not useful
    zoom_range = 0.2,         #useful
    horizontal_flip = True,   #useful
    #vertical_flip = True
    #fill_mode = 'nearest'
    )

"""Early Stopping"""

from keras.callbacks import EarlyStopping, ModelCheckpoint


#Many other options for the early stopping are available. In my opinion this should be enough for our purposes
callback = [EarlyStopping(monitor='val_acc', patience=15),
             ModelCheckpoint(filepath='best_model.h5', monitor='val_acc', save_best_only=True)]
#I am using as the stopping criterion the validation accuracy. Another possibility is the validation loss.
#I am using a really high value for patience. Hard to overtrain the model with optimizator used.

from keras import optimizers
from keras.optimizers import SGD
batch_S = 32
epoc = 500

#opt = SGD(lr=0.1)    #DO NOT USE. Just keep to remember it has been used at first to test the model (plus as a memo to myself since I wasted half an hour trying to get why shit was not working)
opt = optimizers.SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)  #random optimizer found on the documentation. I am open to suggestions

model.compile(optimizer=opt,
            loss='categorical_crossentropy',
            metrics=['accuracy'])

base_model.trainable = True

set_trainable = False
for layer in base_model.layers:
    if layer.name == 'block5_conv1':
        set_trainable = True
    if set_trainable:
        layer.trainable = True
    else:
        layer.trainable = False

history = model.fit_generator(datagen.flow(x_train, y_train,batch_size = 128,shuffle = True),  
                    epochs=epoc,
                    shuffle = True, 
                    steps_per_epoch =  len(x_train)/32,  
                    validation_data = (x_val,y_val),
                    callbacks = callback,
                    validation_steps = 50)

acc = history.history['acc']
val_acc = history.history['val_acc']
loss = history.history['loss']
val_loss = history.history['val_loss']

import matplotlib.pyplot as plt


epochs = range(len(acc))

plt.plot(epochs, acc, 'bo', label='Training acc')
plt.plot(epochs, val_acc, 'b', label='Validation acc')
plt.title('Training and validation accuracy')
plt.legend()

plt.figure()

plt.plot(epochs, loss, 'bo', label='Training loss')
plt.plot(epochs, val_loss, 'b', label='Validation loss')
plt.title('Training and validation loss')
plt.legend()

plt.show()

test_loss, test_acc, test_precision , test_recall= model.evaluate(x_test, y_test)
print(test_acc)
print(test_precision)
print(test_recall)